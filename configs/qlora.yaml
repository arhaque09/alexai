base_model: Qwen/Qwen2.5-3B-Instruct
dataset_path: data/train_ready.jsonl
output_dir: outputs/alex-qlora
adapter_name: alex_tone
bf16: false
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2.0e-4
num_train_epochs: 2
logging_steps: 10
save_steps: 500
warmup_ratio: 0.03
lr_scheduler_type: cosine
packing: true
max_seq_length: 1024
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules: [q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj]
bnb_4bit: false
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: bfloat16
